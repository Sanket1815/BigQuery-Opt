# ğŸ”„ MCP Server - Complete Workflow Flowchart

## ğŸ¯ **What is the MCP Server?**

The **MCP (Model Context Protocol) Server** is the **core workflow orchestrator** that acts as a bridge between:
- **Frontend UI** (user interface)
- **Gemini AI API** (AI optimization)
- **BigQuery** (database execution)
- **Validation Engine** (syntax, schema, and LLM cleanup)

It's essentially the **"brain"** that coordinates the entire SQL optimization process.

## ğŸ”„ **Complete MCP Server Workflow Flowchart**

```mermaid
flowchart TD
    %% Start
    START([ğŸš€ User Submits SQL Query]) --> INIT[ğŸ“¥ MCP Server Receives Request]
    
    %% Initialization Phase
    INIT --> LOAD_ENV[ğŸ”‘ Load Environment Variables<br/>GEMINI_API_KEY, Project Settings]
    LOAD_ENV --> INIT_HANDLER[âš™ï¸ Initialize DirectSQLOptimizationHandler]
    INIT_HANDLER --> VALIDATE_INPUT[âœ… Validate Input Parameters<br/>SQL Query, Project ID, Flags]
    
    %% AI Optimization Phase
    VALIDATE_INPUT --> LOAD_DOCS[ğŸ“š Load Markdown Documentation<br/>From optimization_docs_md/ folder]
    LOAD_DOCS --> CHECK_TOKENS{ğŸ” Check Token Count<br/>Documentation + Query}
    
    %% Token Management Branch
    CHECK_TOKENS -->|Token Count > Limit| TRIM_DOCS[âœ‚ï¸ Apply Token Reduction Strategy<br/>1. Intelligent Selection<br/>2. Minimal Mode<br/>3. Ultra-Minimal Mode]
    CHECK_TOKENS -->|Token Count OK| PREPARE_PROMPT[ğŸ“ Prepare Gemini Prompts]
    TRIM_DOCS --> PREPARE_PROMPT
    
    %% Gemini API Interaction
    PREPARE_PROMPT --> SEND_TO_GEMINI[ğŸ¤– Send to Gemini 2.5 Pro API<br/>System Prompt + User Prompt + Docs]
    SEND_TO_GEMINI --> GEMINI_RESPONSE{ğŸ”„ Gemini Response Status}
    
    %% Gemini Response Handling
    GEMINI_RESPONSE -->|Success| EXTRACT_QUERY[ğŸ“¤ Extract Optimized Query<br/>Parse Gemini Response]
    GEMINI_RESPONSE -->|Failure| GEMINI_FALLBACK[âš ï¸ Apply Fallback Strategy<br/>Return Original Query + Error]
    GEMINI_FALLBACK --> VALIDATION_PIPELINE
    
    %% Multi-Stage Validation Pipeline
    EXTRACT_QUERY --> VALIDATION_PIPELINE[ğŸ” Start Multi-Stage Validation]
    
    %% Step 3: Regex Syntax Fixes
    VALIDATION_PIPELINE --> REGEX_FIX[ğŸ”§ Step 3: Regex Syntax Fixes]
    REGEX_FIX --> FIX_SPACES[ğŸ“ Fix Missing Spaces<br/>WHEREder_date â†’ WHERE der_date]
    REGEX_FIX --> FIX_KEYWORDS[ğŸ”‘ Fix Corrupted Keywords<br/>OR DER BY â†’ ORDER BY]
    REGEX_FIX --> FIX_QUOTES[ğŸ’¬ Fix Unbalanced Quotes<br/>Add missing closing quotes]
    REGEX_FIX --> FIX_COMPARISONS[âš–ï¸ Fix Malformed Comparisons<br/>>= abc â†’ >= 1]
    
    %% Combine Regex Fixes
    FIX_SPACES --> COMBINE_REGEX_FIXES[ğŸ”„ Combine All Regex Fixes]
    FIX_KEYWORDS --> COMBINE_REGEX_FIXES
    FIX_QUOTES --> COMBINE_REGEX_FIXES
    FIX_COMPARISONS --> COMBINE_REGEX_FIXES
    
    %% Step 4: First LLM Cleanup
    COMBINE_REGEX_FIXES --> LLM_CLEANUP_1[ğŸ§  Step 4: LLM Cleanup #1]
    LLM_CLEANUP_1 --> SEND_TO_LLM_1[ğŸ“¤ Send to Gemini for Cleanup<br/>Fix NULL columns, invalid WHERE conditions]
    SEND_TO_LLM_1 --> LLM_RESPONSE_1{ğŸ”„ LLM Response #1}
    LLM_RESPONSE_1 -->|Success| CLEANED_QUERY_1[âœ… Query After First Cleanup]
    LLM_RESPONSE_1 -->|Failure| USE_REGEX_RESULT[âš ï¸ Use Regex-Fixed Query<br/>Skip LLM Cleanup #1]
    USE_REGEX_RESULT --> CLEANED_QUERY_1
    
    %% Step 5: Schema Validation
    CLEANED_QUERY_1 --> SCHEMA_VALIDATION[ğŸ—ï¸ Step 5: Schema Validation]
    SCHEMA_VALIDATION --> CHECK_TABLES[ğŸ“‹ Check Table Existence<br/>Validate FROM and JOIN tables]
    SCHEMA_VALIDATION --> CHECK_COLUMNS[ğŸ” Check Column Existence<br/>Validate SELECT, WHERE, JOIN columns]
    SCHEMA_VALIDATION --> REMOVE_INVALID[âŒ Remove Invalid References<br/>Non-existent tables/columns]
    SCHEMA_VALIDATION --> VALIDATE_JOINS[ğŸ”— Validate JOIN Conditions<br/>Check JOIN syntax and references]
    
    %% Combine Schema Validation
    CHECK_TABLES --> COMBINE_SCHEMA_VAL[ğŸ”„ Combine Schema Validation Results]
    CHECK_COLUMNS --> COMBINE_SCHEMA_VAL
    REMOVE_INVALID --> COMBINE_SCHEMA_VAL
    VALIDATE_JOINS --> COMBINE_SCHEMA_VAL
    
    %% Step 6: Final LLM Cleanup
    COMBINE_SCHEMA_VAL --> LLM_CLEANUP_2[ğŸ§  Step 6: Final LLM Cleanup]
    LLM_CLEANUP_2 --> SEND_TO_LLM_2[ğŸ“¤ Send to Gemini for Final Cleanup<br/>Fix remaining syntax, remove NULL conditions]
    SEND_TO_LLM_2 --> LLM_RESPONSE_2{ğŸ”„ LLM Response #2}
    LLM_RESPONSE_2 -->|Success| FINAL_OPTIMIZED_QUERY[âœ… Final Optimized Query]
    LLM_RESPONSE_2 -->|Failure| USE_SCHEMA_RESULT[âš ï¸ Use Schema-Validated Query<br/>Skip Final LLM Cleanup]
    USE_SCHEMA_RESULT --> FINAL_OPTIMIZED_QUERY
    
    %% Execution Phase
    FINAL_OPTIMIZED_QUERY --> EXECUTION_PHASE[âš¡ Phase 4: Query Execution]
    EXECUTION_PHASE --> EXECUTE_ORIGINAL[ğŸ“Š Execute Original Query<br/>Collect performance metrics]
    EXECUTION_PHASE --> EXECUTE_OPTIMIZED[ğŸš€ Execute Optimized Query<br/>Collect performance metrics]
    
    %% Performance Metrics Collection
    EXECUTE_ORIGINAL --> COLLECT_METRICS_ORIG[ğŸ“ˆ Collect Original Metrics<br/>Time, Bytes, Cost, Rows]
    EXECUTE_OPTIMIZED --> COLLECT_METRICS_OPT[ğŸ“ˆ Collect Optimized Metrics<br/>Time, Bytes, Cost, Rows]
    
    %% Comparison Phase
    COLLECT_METRICS_ORIG --> COMPARISON_PHASE[ğŸ“Š Phase 5: Result Comparison]
    COLLECT_METRICS_OPT --> COMPARISON_PHASE
    
    %% Hash-Based Validation
    COMPARISON_PHASE --> HASH_VALIDATION[ğŸ” Hash-Based Validation]
    HASH_VALIDATION --> GENERATE_HASHES[ğŸ”‘ Generate MD5 Hashes<br/>Original Results + Optimized Results]
    GENERATE_HASHES --> COMPARE_HASHES[âš–ï¸ Compare Hash Values<br/>Check if results are identical]
    
    %% Result Analysis
    COMPARE_HASHES --> RESULT_ANALYSIS[ğŸ“‹ Result Analysis]
    RESULT_ANALYSIS --> ROW_COUNT_CHECK[ğŸ”¢ Row Count Validation<br/>Check if row counts match]
    RESULT_ANALYSIS --> DATA_STRUCTURE_CHECK[ğŸ—ï¸ Data Structure Validation<br/>Check column names, types]
    RESULT_ANALYSIS --> BUSINESS_LOGIC_CHECK[ğŸ’¼ Business Logic Preservation<br/>Check if semantics are preserved]
    
    %% Performance Analysis
    RESULT_ANALYSIS --> PERFORMANCE_ANALYSIS[ğŸ“Š Performance Analysis]
    PERFORMANCE_ANALYSIS --> CALCULATE_IMPROVEMENTS[ğŸ“ˆ Calculate Improvements<br/>Time, Bytes, Cost savings]
    PERFORMANCE_ANALYSIS --> GENERATE_METRICS[ğŸ“Š Generate Performance Metrics<br/>Actual vs Expected improvements]
    
    %% Response Generation
    CALCULATE_IMPROVEMENTS --> GENERATE_RESPONSE[ğŸ“¤ Generate Unified Response]
    GENERATE_METRICS --> GENERATE_RESPONSE
    
    %% Response Structure
    GENERATE_RESPONSE --> RESPONSE_STRUCTURE[ğŸ“‹ Build Response Object]
    RESPONSE_STRUCTURE --> ADD_QUERIES[â• Add Original & Optimized Queries]
    RESPONSE_STRUCTURE --> ADD_RESULTS[â• Add Query Results & Metrics]
    RESPONSE_STRUCTURE --> ADD_VALIDATION[â• Add Validation Results<br/>Hash comparison, preservation score]
    RESPONSE_STRUCTURE --> ADD_PERFORMANCE[â• Add Performance Analysis<br/>Actual improvements, optimization details]
    
    %% Final Response
    ADD_QUERIES --> FINAL_RESPONSE[ğŸ¯ Final Unified Response]
    ADD_RESULTS --> FINAL_RESPONSE
    ADD_VALIDATION --> FINAL_RESPONSE
    ADD_PERFORMANCE --> FINAL_RESPONSE
    
    %% Return to API
    FINAL_RESPONSE --> RETURN_TO_API[ğŸ“¤ Return Response to API Gateway]
    RETURN_TO_API --> END([ğŸ MCP Server Workflow Complete])
    
    %% Error Handling Paths
    GEMINI_FALLBACK --> ERROR_HANDLING[âš ï¸ Error Handling]
    USE_REGEX_RESULT --> ERROR_HANDLING
    USE_SCHEMA_RESULT --> ERROR_HANDLING
    ERROR_HANDLING --> RETURN_TO_API
    
    %% Styling
    classDef startEnd fill:#e8f5e8,stroke:#1b5e20,stroke-width:3px
    classDef mcpProcess fill:#e3f2fd,stroke:#0277bd,stroke-width:2px
    classDef aiProcess fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef validationProcess fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef executionProcess fill:#e0f2f1,stroke:#004d40,stroke-width:2px
    classDef comparisonProcess fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef responseProcess fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef errorProcess fill:#ffebee,stroke:#c62828,stroke-width:2px
    
    class START,END startEnd
    class INIT,LOAD_ENV,INIT_HANDLER,VALIDATE_INPUT,LOAD_DOCS,CHECK_TOKENS,TRIM_DOCS,PREPARE_PROMPT mcpProcess
    class SEND_TO_GEMINI,GEMINI_RESPONSE,EXTRACT_QUERY,SEND_TO_LLM_1,LLM_CLEANUP_1,LLM_RESPONSE_1,SEND_TO_LLM_2,LLM_CLEANUP_2,LLM_RESPONSE_2 aiProcess
    class VALIDATION_PIPELINE,REGEX_FIX,FIX_SPACES,FIX_KEYWORDS,FIX_QUOTES,FIX_COMPARISONS,COMBINE_REGEX_FIXES,LLM_CLEANUP_1,CLEANED_QUERY_1,SCHEMA_VALIDATION,CHECK_TABLES,CHECK_COLUMNS,REMOVE_INVALID,VALIDATE_JOINS,COMBINE_SCHEMA_VAL,LLM_CLEANUP_2,FINAL_OPTIMIZED_QUERY validationProcess
    class EXECUTION_PHASE,EXECUTE_ORIGINAL,EXECUTE_OPTIMIZED,COLLECT_METRICS_ORIG,COLLECT_METRICS_OPT executionProcess
    class COMPARISON_PHASE,HASH_VALIDATION,GENERATE_HASHES,COMPARE_HASHES,RESULT_ANALYSIS,ROW_COUNT_CHECK,DATA_STRUCTURE_CHECK,BUSINESS_LOGIC_CHECK,PERFORMANCE_ANALYSIS,CALCULATE_IMPROVEMENTS,GENERATE_METRICS comparisonProcess
    class GENERATE_RESPONSE,RESPONSE_STRUCTURE,ADD_QUERIES,ADD_RESULTS,ADD_VALIDATION,ADD_PERFORMANCE,FINAL_RESPONSE,RETURN_TO_API responseProcess
    class GEMINI_FALLBACK,USE_REGEX_RESULT,USE_SCHEMA_RESULT,ERROR_HANDLING errorProcess
```

## ğŸ” **Detailed MCP Server Responsibilities**

### **1. ğŸ¯ Workflow Orchestration**
- **Coordinates** all optimization steps
- **Manages** the flow between different phases
- **Handles** state transitions and error conditions
- **Ensures** consistent processing for all request types

### **2. ğŸ¤– AI Integration Management**
- **Initializes** Gemini API client
- **Manages** token limits and documentation loading
- **Handles** AI API failures gracefully
- **Implements** fallback strategies

### **3. ğŸ” Multi-Stage Validation**
- **Orchestrates** the 6-step validation pipeline
- **Coordinates** between regex fixes and LLM cleanup
- **Manages** schema validation against BigQuery
- **Ensures** query quality at each step

### **4. âš¡ Execution Management**
- **Coordinates** BigQuery query execution
- **Collects** performance metrics
- **Handles** execution errors
- **Manages** result collection

### **5. ğŸ“Š Analysis & Comparison**
- **Performs** hash-based result validation
- **Calculates** performance improvements
- **Analyzes** result preservation
- **Generates** comprehensive metrics

### **6. ğŸ“¤ Response Generation**
- **Builds** unified response objects
- **Structures** data for frontend consumption
- **Handles** error cases gracefully
- **Ensures** consistent response format

## ğŸ—ï¸ **MCP Server Architecture Components**

```mermaid
graph TB
    subgraph "MCP Server Core"
        MAIN[ğŸ¯ Main Workflow Controller<br/>DirectSQLOptimizationHandler]
        AI_MGR[ğŸ¤– AI Integration Manager<br/>Gemini Client Management]
        VAL_MGR[ğŸ” Validation Manager<br/>Multi-Stage Pipeline]
        EXEC_MGR[âš¡ Execution Manager<br/>BigQuery Operations]
        COMP_MGR[ğŸ“Š Comparison Manager<br/>Result Analysis]
    end
    
    subgraph "External Dependencies"
        GEMINI[ğŸ¤– Gemini 2.5 Pro API]
        BQ[âš¡ BigQuery Client]
        DOCS[ğŸ“š Markdown Documentation]
        ENV[ğŸ”‘ Environment Variables]
    end
    
    subgraph "Internal Services"
        TOKEN_MGR[ğŸ”¢ Token Management<br/>Documentation Trimming]
        ERROR_HANDLER[âš ï¸ Error Handler<br/>Fallback Strategies]
        RESPONSE_BUILDER[ğŸ“¤ Response Builder<br/>Data Structuring]
        LOGGER[ğŸ“ Logger<br/>Workflow Tracking]
    end
    
    %% Core Flow
    MAIN --> AI_MGR
    MAIN --> VAL_MGR
    MAIN --> EXEC_MGR
    MAIN --> COMP_MGR
    
    %% AI Integration
    AI_MGR --> GEMINI
    AI_MGR --> TOKEN_MGR
    AI_MGR --> DOCS
    
    %% Validation
    VAL_MGR --> AI_MGR
    VAL_MGR --> ERROR_HANDLER
    
    %% Execution
    EXEC_MGR --> BQ
    EXEC_MGR --> ERROR_HANDLER
    
    %% Comparison
    COMP_MGR --> RESPONSE_BUILDER
    
    %% Environment & Logging
    MAIN --> ENV
    MAIN --> LOGGER
    
    %% Response
    MAIN --> RESPONSE_BUILDER
    
    style MAIN fill:#e8f5e8,stroke:#1b5e20,stroke-width:3px
    style AI_MGR fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style VAL_MGR fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    style EXEC_MGR fill:#e0f2f1,stroke:#004d40,stroke-width:2px
    style COMP_MGR fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    style TOKEN_MGR fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style ERROR_HANDLER fill:#ffebee,stroke:#c62828,stroke-width:2px
    style RESPONSE_BUILDER fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    style LOGGER fill:#f1f8e9,stroke:#33691e,stroke-width:2px
```

## ğŸ¯ **Key MCP Server Features**

### **ğŸ”„ Unified Processing**
- **Single Entry Point**: All optimization requests go through the same workflow
- **Consistent Behavior**: Identical processing for single queries and test suites
- **Predictable Results**: Same validation and execution logic

### **ğŸ›¡ï¸ Robust Error Handling**
- **Graceful Degradation**: Continues processing even if some steps fail
- **Fallback Strategies**: Multiple approaches for handling failures
- **Comprehensive Logging**: Detailed tracking of all operations

### **âš¡ Performance Optimization**
- **Token Management**: Intelligent documentation loading
- **Parallel Processing**: Where possible, operations run concurrently
- **Resource Management**: Efficient use of API calls and database connections

### **ğŸ” Quality Assurance**
- **Multi-Stage Validation**: Comprehensive query checking
- **Result Preservation**: Ensures optimized queries return identical results
- **Performance Monitoring**: Real-time metrics collection and analysis

The MCP Server is essentially the **"command center"** that makes the entire BigQuery optimization system work seamlessly! ğŸš€ 